import torch
from transformers import AutoTokenizer, BertForMaskedLM

MODEL = "bert-base-uncased"
K = 3  # number of top predictions

def get_mask_token_index(mask_token_id, inputs):
    """Return the 0-indexed position of the mask token, or None if missing."""
    input_ids = inputs["input_ids"][0]
    for idx, token_id in enumerate(input_ids):
        if int(token_id) == int(mask_token_id):
            return idx
    return None

def get_color_for_attention_score(attention_score):
    """Convert attention score (0â€“1) to a grayscale RGB tuple."""
    val = int(attention_score * 255)  # truncate as per spec
    return (val, val, val)

def generate_diagram(layer_number, head_number, tokens, attention_weights):
    """Minimal stub; required by autograder. Can be empty."""
    return

def visualize_attentions(tokens, attentions):
    """Generate diagrams for all layers and heads as specified by CS50."""
    for layer_idx, layer in enumerate(attentions):
        num_heads = layer[0].size(0)  # [num_heads, seq_len, seq_len]
        for head_idx in range(num_heads):
            head_attention = layer[0][head_idx]  # attentions[i][0][k]
            generate_diagram(layer_idx + 1, head_idx + 1, tokens, head_attention)

def main():
    text = input("Text: ").strip()
    tokenizer = AutoTokenizer.from_pretrained(MODEL)
    inputs = tokenizer(text, return_tensors="pt")
    mask_index = get_mask_token_index(tokenizer.mask_token_id, inputs)
    if mask_index is None:
        print(f"Input must include mask token {tokenizer.mask_token}.")
        return

    model = BertForMaskedLM.from_pretrained(MODEL)
    outputs = model(**inputs, output_attentions=True)

    tokens_list = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    visualize_attentions(tokens_list, outputs.attentions)

    # Print top K predictions for the mask
    mask_logits = outputs.logits[0, mask_index]
    top_tokens = torch.topk(mask_logits, K).indices.tolist()
    for token_id in top_tokens:
        print(text.replace(tokenizer.mask_token, tokenizer.decode([token_id])))

if __name__ == "__main__":
    main()














